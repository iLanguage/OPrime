<html>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<link rel="stylesheet" type="text/css" href="css/oprime.css">
<link rel="stylesheet" type="text/css" href="modules/experiment/menu.css">
<link href='https://fonts.googleapis.com/css?family=Merienda+One'
	rel='stylesheet' type='text/css'>
<script src="modules/experiment/menu.js"></script>
</head>

<body>
	<table cellpadding="0" cellspacing="0" border="0" width="100%">
		<tr valign="top">
			<td>
				<!-- <canvas id="homeCanvas"> </canvas>-->
				<canvas id="a" style="z-index: -1, background:    #fff, top:    60;"
					width="10" height="10">
</canvas>
			</td>
			<td id="sidebar" width="%50">
				<h1>Welcome to our Bilingual Aphasia Test - Administrator's
					console !</h1>

				<p>We built this user interface to visualise the data collected
					from the Android Bilingual Aphasia Test which we presented at the
					Academy of Aphasia Annual Meeting Oct 15-18 2011 in Montreal.</p>

				Options:


				<ul>
					<li>We made a HTML5 app which can load touch data and display
						it on the stimuli images in response times, and also "auto-grade"
						normal answers for those stimuli. The included data is from 17
						Highly Proficient Bilingual university students tested in
						Montreal. You can click on one of the SubExperiments on the left
						to view the touch results, or you can open the<a
						href="touch_response_visualizer.html">Touch Data Visualizer</a>
					<li>We made a script which can extract all the touch data from
						the subtitles in the results folder and put it into a format that
						the Touch Data Visualizer can read. <a href="extract/touch">Extract
							Touch Data from Your Results</a>
					</li>
					<li>We incorporated a Praat script which performs syllable
						timing analysis on the acoustic data from the results videos. You
						can use this script to get annotated Praat TextGrids whcih you can
						use for transcription later, as well as to statistically analyize
						the participant's speech rate, the total amount of speaking
						(phonation) and the total length of a sub experiment. You can also
						incorporate other Praat scripts into the script. <a
						href="extract/acoustic">Extract Acoustic Data from Your
							Results</a>
					</li>

					<!--
	<li> We incorporated a Sphinx transcriber (tested on Linux, not tested on Mac yet) which uses the Wall Street Journal corpus to guess what the participant said. The transciptions are pretty bad but you can still use it. <a href = "extract/transcription">Extract Transcription Data from Your Results</a></li>

 <li> We incorporated a OpenCV (Open Computer Vision) eye recognizer which tries to identify participant's eyes. This can be used to create cartoon eyes to anomize the data (if you like to program, we are recruiting voluteers to create more eye gaze analysis) </li>
-->
				</ul> We are looking for more creative and interesting ways to
				incorporate other Open Source projects into our data extraction and
				data analysis! <a
				href="https://github.com/iLanguage/OPrimeAdministrator/issues">User
					our Issue Tracker on GitHub to let us know if you have any
					suggestions or comments</a>
				<center>
					<p>&nbsp;</p>
					<span style="font-size: .75em;"
						style="font-family: helvetica, sans-serif;" id="buttonArea">
						<div id='layer1' style='z-index: 10, background:    #ffa;'>
							<!--<input type="button" name="groovybtn1" id="loadButton" class="groovybutton" title="" value="Load" onClick="initCanvas()" />
-->
						</div>
					</span>
				</center>
			</td>
		</tr>
	</table>

</body>

</html>
